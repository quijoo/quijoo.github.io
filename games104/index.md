# Games104 笔记


# Dynamic Global lllumination(GI)
## Ray Tracing
 先上祖师爷
 $$
L_0(x, \omega_0) = L_e(x,\omega_0) + \int_{H^2}f_r(x,\omega_0,\omega_i)L_i(x,\omega_i)\cos\theta_id\omega_i
$$

$$
outgoing = emitted + reflected
$$

光照中最困难的部分就是积分，一个trick的方法是 **蒙特卡洛积分**，基于此就产生了**蒙特卡洛光线追踪**
**原理：**
从像素点打出光纤，碰到物体做一次bounce，随着bounce次数增加，计算量是指数级的
**缺点：**
1. 量的地方很少，只有很少机会打中发光点
2. 计算量大
3. 采样次数越多，噪点越少
难点：采样

tips：所以 Lumen 重点解决的就是采样
**采样**
1. 均匀采样
2. 概率分布函数（按照PDF分布）采样
    就是按照概率分布函数来采样，这样能更好的计算积分，朴素的来说，就是**往更亮的地方发射更多的光线**
    例如：对于天空采样，那么使用 cos 函数（方向靠近正上方发射更多的射线）将会获得噪声更少的图像
    例如：对于光暗差别很大的场景使用 GGX PDF 可以获得对比度更强的光照效果
    但是这些都是特例，不具有普遍意义，这给我们的启示是 **要往亮的地方发射更多的射线**

但是上述的算法都是离线计算（用于电影动画广告？中的），但是对于游戏来说，需要一种 realtime 的方法
## Lumen 的起始点
Lumen 这一系列方法的基本思路就是 Photon Mapping
思路如下：
提出从光源发射光子，光子最终停留在物体表面，最终渲染就是根据物体表面光子的分布来计算最终颜色

Reflective Shadow Maps(RSM, 2005) 这个工作是这一系列方法的开山鼻祖
### RSM 方法

原文链接 ：[Reflective Shadow Maps](https://users.soe.ucsc.edu/~pang/160/s13/proposal/mijallen/proposal/media/p203-dachsbacher.pdf)

渲染物体是从相机视角去看世界的，渲染阴影的时候是从光的视角去看世界的。
**Reflective Shadow Maps** 的意思就是：从光的角度去看世界，记录所有**被照亮**的表面，这个表面就是不考虑光线追踪的情况下，光子第一次到达的场景。

空间所有中第一次被照亮的点会散射出光线照亮其他物体，那么在渲染任何一个眼睛看到的点的时候，只需要将所有散射出来的 radiance 收集起来即可。
也就是说对于每一个**需要被渲染的像素点**， 我们知道所有能照亮该点的**亮点（在光的第一次传播被照亮的点）**，那么计算被渲染点和亮点之间的距离即可知道光照强度。（这个牛逼啊）

**缺点：是渲染复杂度太高**

**优化1** 使用了 Cone Tracing 来优化算法，就是发射许多圆锥去照亮点，如果 Cone 中的 RSM 越多那么就越亮，大概是这么个意思。后来还有更好的方法例如：

 **对 RSM 做 MIP**，对于漫反射面使用较大的圆锥去采样，对于光滑的表面用角度较小的圆锥去采样。
 
**优化2** 因为间接光照是低频的，所以在屏幕空间可以不用逐像素采样，而是采用 逐 k 像素采样，在 Lumen 中也有类似的思想（原来这么早就有了，看来Lumen是个大缝合怪）

**优化3** Cone Tracing 采样时，可以认为 **空间位置相差太大** 或者 **法线朝向相差太大** 的点被认为是**无效点**，就不采样，而是在第一次样样结束后进行一次**单独采样**，类似的 在 Lumen 中对于稀疏采样不work的点，会进行一次单独的加强采样采样（这个也是抄的）

**总结** 
将光子注入到世界的方法
但是没有想到 MIP方法
在低分辨率稀疏点采集间接光照进行插值，但是在特殊区域进行加强采样
使用 Cone 进行蒙特卡洛积分
没有检测遮挡
只能解决单次传播
**RSM 是非常具有启发性的算法**

### Light Propagation Volumes(LPV)
上一个算发解决了将光子注入空间，这个算法解决**光子在空间中传播**，虽然是一个非常具有**启发性**的算法，但是在**数学上有点问题**
重点是 **Propagation**， 但是想法很淳朴：

将空间划分为很多 Voxel，对于每个 Voxel，接收到的光照是一个球面分布，球面无论采集多少个点都可以等价为加权累积到一起，可以用SH（双曲正弦函数）函数表达，这里 在GAMES 202 中会详细的讲，这个104讲得很笼统，不好理解（之后补202的时候仔细搞搞）。

这个方法老师也没看懂，我肯定看不懂了，物理学意义是数学意义都点问题。

### Sparse Voxel Octree for Real-time Global IIIumination(SVOGI)

Nvidia 的工作，这个想法也很淳朴，解决对空间划分的粗细的问题，有以下事实：
1. 有的 Voxel 中没有物体是空的
2. 有的 Voxel 中是实心物体，只有表面有光

使用硬件的**保守光栅化**：对于很小很薄的三角形保证至少有一个 Voxel 在里边，也就是说保证再小再薄的三角形也能被体素化，那就能收集所有表面的 Voxel，那就是用八叉树来对空间划分，但是在实现中 八叉树每个节点不止存子节点和父节点，还存了三个相邻节点，这使得做 过滤，抗锯齿的时候很方便，所以这个数据结构很复杂。

这里有一个巧妙的思想：
使用 Cone 进行采样，而空间是类似 MIP 的八叉树划分，那么很近的地方，Cone 采样到的 Voxel 很少，但是很远的 地方Cone 采样到的Voxel就很多，而 MIP 的思想中，越靠近根节点的 一个 Voxel 就是很多小的 Voxel 的平均，这样就很巧妙的解决了 Cone 采样，越远点越多的情况。

很多算法细节无法考证，并且八叉树在GPU 上不好表达，而且现在已经没人用这个算法了。

### Voxelization Based Global IIIumination(VXGI)
对于 GI 来说，是眼睛看到的区域最重要的部分，并且对于近处进行高精度采样，对远处进行低精度采样。
那么就是近距离的Voxel 很小，远处的 Voxel 很大，就行了。这种方法实现起来清晰明确简单，并且是 GPU 友好的。

**trick1** 循环UV，这样不会出错，但是不太懂这个的原理

**trick2** 按照距离来确定 Voxel 大小会导致屏幕空间上不管近远的 Voxel 大小都是几乎一致的。相当于 "地月距离和地日距离400倍，月球大小和太阳大小400倍，所以导致太阳月球看起来一样大。"  这样就很好,很一致！！

那么光穿过 Voxel 能穿过多少呢？往往体素不是纯黑和纯透明的，而是有一定的透光率。
所以可以将体素中的实体投影到各个方向按照面积计算各个方向的透光率（三个方向）
那么就可以按照以下步骤来渲染
1. 光线注入，计算每一个被直接照亮的 Voxel
2. 对于屏幕上的每一个像素进行 Cone Tracing(可以不用每一个，可以 稀疏采样 + 加强采样)
3. 基于 BRDF 生成采样圆锥，如果表面比较粗糙生成均匀的多个圆锥，如果表面比较光滑生成角度比较集中的圆锥
4. 圆锥越远的时候使用 ClipMap 越上层的数据（近大远小）
5. 光线传播累计不透明度，完全不透明后就不再传播

**问题：**
1. 透明度累计时，直接计算乘法不合理，可能光经过一层左半遮挡，经过一层右半遮挡就全挡完了，但是乘法就还剩0.25，会产生漏光现象

在这种注入光子的思路中，该方法时集大成者！！！下面是另一种思路的算法，屏幕空间全局光照

### Screen Space Global IIIumination(SSGI)
最开始 在 GDC 上讲的方法，在计算反射时，例如光滑的地面反射墙壁，我们要渲染地面上的某个点，那么知道地面的法线，知道相机的方向，计算视线投射到地面时的出射方向即可，再利用这个方向去查询第一个碰到的物体。查询碰撞的方法可以使用GPU提供的屏幕空间的碰撞。

Hierachical Tracing DX12支持，他会自动将我的Z-buffer做成 mips，mip的下层都包裹着上层，然后利用类似二分的方法来做碰撞检测。大致思路是这样的，复杂度是log的，大致的思路是这样的，但是细节不清楚。

并且计算反射的 Light 是可以 Reuse 的，就是前一次光照得到点会成为后一次计算点的一个新的灯泡。大概理解，细节不理解。。。

优点：处理很光滑的物体，效果很好，有很多BUG，但是非常有用，处理很近的contact shadow，hit点很准，场景复杂度无关(仅基于深度图)，处理动态物体

## Lumen（重点，简称左脚踩右脚渲染）
"Ray Tracing  is slow" 所以需要 Lumen
"Sampling is hard"     所以需要 Lumen
Low-res filtered scene space probes lit full pexels

### Phase1：Fast Ray Trace in Any Hardware

Signed Distance Field(SDF) 有向距离场，物体的 外部为+，表面为0，内部为-

SDF 和 顶点在本质上是对偶的
但是SDF 有很多优良性质
1. 均匀的
2. 连续的（可微的）

先对 Mesh 做SDF 再对空间做SDF

SDF 支持统一的线性变换

对于很细的面，难以表达，所以稍微扩张一下

光纤可以很快的找到表面，光一步步向物体推进，当靠近表面时，SDF 会告诉我们应该走多大的步长，当到达表面时是0，当进入表面后，这个值是负数，又能够退回到表面。

做 Cone Tracing 时，可以预估目标的距离（按照圆的比值）：这个不懂

Mesh SDF LoD（多细节层次）

SDF 导数是表面法线

做 MIP 节省40% 空间

Global SDF：将所有物体的SDF 合成一个低精度的场景的SDF
1. 合成算法（由Mesh合成场景）
2. 更新算法（物体是移动的）

总的来说就是，使用 SDF 来表达每一个Mesh 和场景，这样带来了很多计算上的好处，它能提高 Ray Tracing 的效率，从而不依赖于硬件的 Ray Tracing

### Phase2 Radiance Injection and Caching

**这一部分解决：如何抓住光的？**

**Mesh card** 就以相机为原点，对场景中所有物体进行快照，记录该物体 AABB Box 对应的各个面的光照信息（法线，光照，自发光，等），并且这个快照是LoD的（近处高分辨率，远处低分辨率）

**Surface Cache：**分配一个 4096x4096的空间，将每一个 Instance 的Card放到空间里，随着相机的移动，可能会替换（硬件提供方法来压缩），是对直接光照信息的一个表达（将光子固化在表面），在相机移动和物体移动时采用**脏更新**。

**Lighting Cache Pipeline：** 
1. 做逐像素的直接光照（做shadow map，不会错的离谱）
    知道 Mesh 的各种光线相关的图，并且通过 SDF 可以很快的判断遮挡信息，所以可以很快的计算出直接光照图
    对于多个光源，累加在一起即可
2. 在场景中建立 Voxelize 表达，并且将当前帧的光照信息传递给下一帧
    建立体素，来采样，本来我们的SDF是近距离精准，远距离模糊的，远距离模糊那怎么看有没有打到物体上呢？就应该使用 Global SDF，          但是 Global SDF 只能定位到像素点，而不能定位到具体某个 Instance。
    Lumen的解决方法是：对于近处的物体，可以精准的拿到光照信息，对于远处的的物体：以相机为中心做一个 Voxelize的表达，然后 采样光线只要在Global SDF中Hit到了，就直接给那个位置的 Voxelize 的光照信息。
    Lumen的具体方法：
    做一个 ClipMap，四层，每一层 64x64，一个Voxel 是0.78米，每个Voxel的6个面存储亮度，这样在对某个点光照信息积分的时候，发出的光纤就能永远的cast到一个值，这里的 voxel 6个面是只有一个值，而不是光场。这里有点迷惑。那怎么来照亮别人呢？
     
3. 在下一帧中融合表面直接光照和由上一帧继承而来的光照图，得到真实光照
    虽然每次都只做了一次bounce但是在数学上可以推导，实际上就是做了多次bounce（GAMES202 会推导）

Tips:一些限制
直接光照不超过 1024x1024 texture
间接光照不超过 512x512    texture 
选择页来更新，基于 Priority = LastUsed - LastUpdated

### Phase3：Screen Probe structure

怎么区分布Prob，对于屏幕上的每一个点如何使用 Prob去lighting

采样使用**八面体球面采样**（几乎是均匀的），并且插值与球面几乎一致

屏幕空间放置采样点，有一个问题，就是真实的场景中相邻屏幕空间采样点可能是相距很远的。他们并不能很好的插值。
所以应该添加真实距离作为权重，而如果两个相距超过某个阈值，那么就应该申请细分探针数量

如果将屏幕空间的采样点打印出来，会发现所有几何变化较大的地方都会进行细分探针。

**采样**
需要进行importance sampling，就会出现黑一块白一块。
最重要的是，需要朝“窗户”的地方多射一点。怎么知道光在哪里呢？
可以采集上一帧的光的位置（假设光的变化不是那么大）
采样的时候需要按照重要性分布来采样，也就是往光多的地方采样，并且重要性还与法向方向有关

巴拉巴拉，没听懂我操。直接听大概意思吧。不学了。


### Phase4
拿到一个屏幕像素，如何使用Prob来做shading


