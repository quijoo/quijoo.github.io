<!DOCTYPE html>
<html lang="zh-CN" color-mode="light">

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Quijoo" />
  <!-- Open Graph Description 简短摘要-->
  
  <!-- 用于搜索引擎的文章摘要 -->
  
  
  
  <title>
    
      神经网络基础 
      
      
      |
    
     Quijoo 记录
  </title>

  
    <link rel="apple-touch-icon" href="/images/favicon.png">
    <link rel="icon" href="/images/favicon.png">
  

  <!-- Raleway-Font -->
  <link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet">

  <!-- hexo site css -->
  <link rel="stylesheet" href="/css/main.css" />
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1886449_67xjft27j1l.css" />
  <!-- 代码块风格 -->
  

  <!-- jquery3.3.1 -->
  
    <script defer type="text/javascript" src="/plugins/jquery.min.js"></script>
  

  <!-- fancybox -->
  
    <link href="/plugins/jquery.fancybox.min.css" rel="stylesheet">
    <script defer type="text/javascript" src="/plugins/jquery.fancybox.min.js"></script>
  
  
<script src="/js/fancybox.js"></script>


  

  

  <script>
    var html = document.documentElement
    const colorMode = localStorage.getItem('color-mode')
    if (colorMode) {
      document.documentElement.setAttribute('color-mode', colorMode)
    }
  </script>
<meta name="generator" content="Hexo 8.1.1"></head>


  <body>
    <div id="app">
      <div class="header">
  <div class="avatar">
    <a href="/">
      <!-- 头像取消懒加载，添加no-lazy -->
      
        <img src="/images/avatar.png" alt="">
      
    </a>
    <div class="nickname"><a href="/">Oranges</a></div>
  </div>
  <div class="navbar">
    <ul>
      
        <li class="nav-item" data-path="/">
          <a href="/">Home</a>
        </li>
      
        <li class="nav-item" data-path="/archives/">
          <a href="/archives/">Archives</a>
        </li>
      
        <li class="nav-item" data-path="/tags/">
          <a href="/tags/">Tags</a>
        </li>
      
        <li class="nav-item" data-path="/friends/">
          <a href="/friends/">Friends</a>
        </li>
      
        <li class="nav-item" data-path="/about/">
          <a href="/about/">About</a>
        </li>
      
    </ul>
  </div>
</div>


<script src="/js/activeNav.js"></script>



      <div class="flex-container">
        <!-- 文章详情页，展示文章具体内容，url形式：https://yoursite/文章标题/ -->
<!-- 同时为「标签tag」，「朋友friend」，「分类categories」，「关于about」页面的承载页面，具体展示取决于page.type -->


  <!-- LaTex Display -->

  
    <script async type="text/javascript" src="/plugins/mathjax/tex-chtml.js"></script>
  
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    }
  </script>





  <!-- clipboard -->

  
    <script async type="text/javascript" src="/plugins/clipboard.min.js"></script>
  
  
<script src="/js/codeCopy.js"></script>







  

  

  

  
  <!-- 文章内容页 url形式：https://yoursite/文章标题/ -->
  <div class="container post-details" id="post-details">
    <div class="post-content">
      <div class="post-title">神经网络基础</div>
      <div class="post-attach">
        <span class="post-pubtime">
          <i class="iconfont icon-updatetime mr-10" title="更新时间"></i>
          2025-12-13 06:32:43
        </span>
        
      </div>
      <div class="markdown-body">
        <p><a target="_blank" rel="noopener" href="http://xuaii.github.io//L%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">L层神经网络.html</a></p>
<h1 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h1><h2 id="实现方式：采用BP算法，使用梯度下降来优化W，b"><a href="#实现方式：采用BP算法，使用梯度下降来优化W，b" class="headerlink" title="实现方式：采用BP算法，使用梯度下降来优化W，b"></a>实现方式：采用BP算法，使用梯度下降来优化W，b</h2><h2 id="需要实现的函数如下："><a href="#需要实现的函数如下：" class="headerlink" title="需要实现的函数如下："></a>需要实现的函数如下：</h2><p><code>initial()</code>:根据用户自定义的层<code>layer</code>-&gt;[ $n_1, n_2,n_3,…,n_m$ ]来初始化参数W，b</p>
<p><code>sigmoid()</code>:输入$X \in R^{m x n}$ ，返回$A &#x3D; \frac{1}{1+e^{-WX + b}}$</p>
<p><code>liner_propagation()</code>:实现一层的前向传播，返回 A，并且缓存中间量Z </p>
<p><code>L_layers_propagation()</code>:实现L层的前向传播，并且缓存所有中间层的A，Z</p>
<p><code>predict()</code>:使用训练好的模型Ws，bs预测某一输入特征向量对应的预测值</p>
<p><code>back_propagation()</code>:实现一层的反向传播</p>
<p><code>L_back_propagation()</code>:实现L层链式反向传播</p>
<p><code>update_parameters()</code>:每一个epoch更新参数</p>
<p><code>evaulate()</code>:评估模型</p>
<p><code>model()</code>:主循环，BP算法梯度下降的循环</p>
<p><code>load_data()</code>:加载数据</p>
<p><strong>在整体的设计中并没有loss函数的出现，是因为，在反向传播过过中，dloss&#x2F;dW的计算并不涉及loss的值，dloss&#x2F;dW的表达式中仅有y_truth 和 y_pred</strong></p>
<h1 id="链式偏微分-推导（这里是根据西瓜书上原始公式推导，所以是累计神经网络）"><a href="#链式偏微分-推导（这里是根据西瓜书上原始公式推导，所以是累计神经网络）" class="headerlink" title="链式偏微分 推导（这里是根据西瓜书上原始公式推导，所以是累计神经网络）"></a>链式偏微分 推导（这里是根据西瓜书上原始公式推导，所以是累计神经网络）</h1><h2 id="由于在布置编程作业之前就自己实现了一下神经网络，所以实现的是任意层数和任意数量神经元的神经网络，所以就改下参数交作业了"><a href="#由于在布置编程作业之前就自己实现了一下神经网络，所以实现的是任意层数和任意数量神经元的神经网络，所以就改下参数交作业了" class="headerlink" title="由于在布置编程作业之前就自己实现了一下神经网络，所以实现的是任意层数和任意数量神经元的神经网络，所以就改下参数交作业了"></a>由于在布置编程作业之前就自己实现了一下神经网络，所以实现的是任意层数和任意数量神经元的神经网络，所以就改下参数交作业了</h2><ul>
<li>由于神经网络层数可能会非常多，所以在反向传播时loss对每一层的W，b求导会重复很多中间步骤<br> $$设：\beta^i \in R^{m \cdot n}$$<br>$$ A^{i-1} \in R^{n \cdot s}$$<br>$$ Z^{i} \in R^{m \cdot s}$$</li>
</ul>
<p>$$Z^i &#x3D; W^i A^{i-1} + b^i &#x3D; \beta^i A^{i-1}, $$ </p>
<p>$$A^i &#x3D; sigmoid(Z^i);$$</p>
<p>$$\frac{\partial E}{\partial \beta^i}  &#x3D; \frac{\partial Z^i}{\partial \beta^i} \cdot  \underbrace{ \frac{\partial A^i}{\partial Z^i} \cdot  \overbrace {\frac{\partial E}{\partial A^i}}^{\partial A^i} }_{\partial Z^i}  ， \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad  （1.1）$$</p>
<p>$$ \frac{\partial E}{\partial A^{i-1}}  &#x3D; \frac{\partial Z^i}{\partial A^{i-1}}  \cdot \underbrace {\frac{\partial A^i}{\partial Z^i}  \cdot  \overbrace{  \frac{\partial E}{\partial A^i} }^{\partial A^i} }_{\partial Z^i}, \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad  （1.2）$$</p>
<p>其中：<br>$$\partial A^i由上一层反向传播提供,$$ </p>
<p><strong>现在考虑每一层反向传播的计算</strong></p>
<p><code>已知:</code><br>$$\frac{\partial E}{\partial A^k}, \frac{\partial A^k}{\partial Z^k}, \qquad其中：k是层数$$</p>
<p><code>求解:</code><br>$$\frac{\partial E}{\partial {Z^k}_{i,j}}  \qquad其中： i,j \in { i,j | i \in (1,Z^k.shape[0]， j \in (1,Z^k.shape[1])}$$</p>
<p><code>分析：</code></p>
<p><em><strong>由于 （暂时忽略掉上标，仅在需要的时候添加）</strong></em><br>$$Z &#x3D; \beta A$$<br>$$Z_{i,j} &#x3D; \sum_{k&#x3D;1}^{n}\beta_{i,k}A_{k,j}$$<br><strong>该式子表明</strong><br>$$Z_{i,j}的值与同行\beta{i,k}$$</p>
<p>$$即每一个\beta与Z的第a行相关$$<br>$$那么要计算\frac{\partial E}{\partial \beta_{a,b}}$$</p>
<p>$$即需要\frac{\partial E}{\partial \beta_{a,b}} &#x3D; \sum_{k&#x3D;1}^{s}\frac{\partial E}{\partial Z_{a,k}} \frac{\partial Z_{a,k}}{\partial \beta_{a,b}}$$<br>$$而：\frac{\partial Z_{a,k}}{\partial \beta_{a,b}} &#x3D; A_{b,k}$$<br>$$那么：\frac{\partial E}{\partial \beta_{a,b}} &#x3D; \sum_{k&#x3D;1}^{s}\frac{\partial E}{\partial Z_{a,k}} \cdot A_{b,k}$$<br>$$\frac{\partial E}{\partial \beta_{a,b}} &#x3D; \sum_{k&#x3D;1}^{s} {(\frac{\partial E}{\partial Z})}<em>{a,k} \cdot A</em>{b,k}$$<br>$$ &#x3D; \sum_{k&#x3D;1}^{s}(\frac{\partial E}{\partial Z})<em>{a,k}(A^T)</em>{k,b}$$<br>$$所以有：\frac{\partial E}{\beta} &#x3D; dZ A^T$$</p>
<p><strong>由于推导的迭代式子中包含A, Z, 等前向传播产生的中间结果， 所以在前向传播时需要将它们缓存下来</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.animation <span class="keyword">import</span> FuncAnimation</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc, accuracy_score, confusion_matrix, classification_report, roc_auc_score</span><br><span class="line"><span class="comment"># 设置随机种子，避免每次的结果不一样</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="初始化函数initial-layers"><a href="#初始化函数initial-layers" class="headerlink" title="初始化函数initial(layers)"></a>初始化函数<code>initial(layers)</code></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input: layers -&gt; [layer_1, layer_2, layer_3, ..., layer_m]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output: paramters -&gt; <span class="built_in">dict</span>()  </span><br><span class="line">    parameters[<span class="string">&quot;w&quot;</span>] = temp_w -&gt; <span class="built_in">list</span>()</span><br><span class="line">    parameters[<span class="string">&#x27;layers&#x27;</span>] = layers -&gt; <span class="built_in">list</span>()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initial</span>(<span class="params">layers</span>):</span><br><span class="line">    parameters = <span class="built_in">dict</span>()</span><br><span class="line">    temp_w = <span class="built_in">list</span>()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(layers)):</span><br><span class="line">        temp_w.append(np.random.randn(layers[i], layers[i-<span class="number">1</span>]))</span><br><span class="line">    parameters[<span class="string">&quot;w&quot;</span>] = temp_w</span><br><span class="line">    parameters[<span class="string">&#x27;layers&#x27;</span>] = layers</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<h2 id="激活函数-Sigmoid"><a href="#激活函数-Sigmoid" class="headerlink" title="激活函数 Sigmoid()"></a>激活函数 Sigmoid()</h2><p><strong>激活函数种类：</strong></p>
<p><code>sigmoid()</code>:”S”形函数$f(x) &#x3D; \frac{1}{1+e^{-WX + b}}$$</p>
<p><code>ReLU()</code>:线性修正单元$f(x) &#x3D; max(0,x)$</p>
<p><code>tanh()</code>:双曲正切函数$f(x) &#x3D; \frac{e^x-e^{-x}}{e^x+e^{-x}}$</p>
<p><code>ELU</code>:</p>
<p><img src="https://img-blog.csdn.net/20180104121237935?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2FuZ3lpNDEx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
<p><code>PReLU</code>:</p>
<p><img src="https://img-blog.csdn.net/20180104115618817?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2FuZ3lpNDEx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
<p><code>LReLU</code>:x负半轴斜率比较小的PReLU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">A</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-A))</span><br></pre></td></tr></table></figure>

<h2 id="前向线性传播"><a href="#前向线性传播" class="headerlink" title="前向线性传播"></a>前向线性传播</h2><p>$Z &#x3D; WX + b$</p>
<p>$A &#x3D; \frac{1}{1+e^-Z}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">liner_propagation</span>(<span class="params">w, A</span>):</span><br><span class="line">    Z = np.dot(w, A)</span><br><span class="line">    A = sigmoid(Z)</span><br><span class="line">    <span class="keyword">return</span> Z, A</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="L层的前向传播"><a href="#L层的前向传播" class="headerlink" title="L层的前向传播"></a>L层的前向传播</h2><ul>
<li>是L次<code>linear_propagation</code>的叠加</li>
<li>由于反向传播的需要，这里的L层前向传播需要缓存每一层的A，Z到<code>A_cache</code> and <code>Z_chache</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L_layers_propagation</span>(<span class="params">parameters, x</span>):</span><br><span class="line">    A_cache = <span class="built_in">list</span>()</span><br><span class="line">    Z_cache = <span class="built_in">list</span>()</span><br><span class="line">    cache = <span class="built_in">dict</span>()</span><br><span class="line">    A_cache.append(x)</span><br><span class="line">    Z_cache.append(x)</span><br><span class="line">    w = parameters[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">    layers = parameters[<span class="string">&#x27;layers&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(layers) - <span class="number">1</span>):</span><br><span class="line">        Z, A = liner_propagation(w[i], A_cache[i])</span><br><span class="line">        <span class="keyword">assert</span> Z.shape == A.shape</span><br><span class="line">        Z_cache.append(Z)</span><br><span class="line">        A_cache.append(A)</span><br><span class="line">        </span><br><span class="line">    cache[<span class="string">&quot;A&quot;</span>] = A_cache</span><br><span class="line">    cache[<span class="string">&quot;Z&quot;</span>] = Z_cache</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cache</span><br></pre></td></tr></table></figure>

<h3 id="Loss函数（均方误差）"><a href="#Loss函数（均方误差）" class="headerlink" title="Loss函数（均方误差）"></a>Loss函数（均方误差）</h3><ul>
<li>如果要实现任意loss函数，只会影响链式传播哦的最后一项，而不会影响链式传播的中间过程</li>
<li>由于最近DDL多，所以任意loss函数的实现，在寒假实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lossFunc</span>(<span class="params">Y_pre, Y_true, method = <span class="string">&quot;MSE&quot;</span></span>):</span><br><span class="line">    temp = Y_pre - Y_true</span><br><span class="line">    <span class="keyword">return</span> <span class="number">.5</span> * np.dot(temp, temp.T)</span><br></pre></td></tr></table></figure>

<h2 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a>预测函数</h2><p><code>input = x</code></p>
<p><code>output = y</code></p>
<p>$y &#x3D; f(x)$</p>
<p>$y&gt;.5$          &#x3D;&gt;      $y&#x3D;1$</p>
<p>$y\le .5$       &#x3D;&gt;      $y&#x3D; 0$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">parameters, x, draw = <span class="literal">False</span></span>):</span><br><span class="line">    Y = L_layers_propagation(parameters, x)[<span class="string">&#x27;A&#x27;</span>][-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> draw == <span class="literal">False</span>:    </span><br><span class="line">        Y[Y &gt; <span class="number">.5</span>] = <span class="number">1</span></span><br><span class="line">        Y[Y &lt;= <span class="number">.5</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>

<h2 id="反向传播函数"><a href="#反向传播函数" class="headerlink" title="反向传播函数"></a>反向传播函数</h2><p><code>back_propagation</code></p>
<p><strong>实现反向传播的难点在于求梯度</strong></p>
<p><strong>求梯度的方法如下：</strong></p>
<ol>
<li>$dx &#x3D; \frac{f(x) - f(x-\Delta x)}{\Delta x}$<ul>
<li>老师上课提到的这种方法会导致精确度问题</li>
<li>如当在梯度下降算法中设定$\Delta x&#x3D; \eta$时该算法退化为 <code>W := W - loss(x-learning_rate)</code></li>
</ul>
</li>
</ol>
<p>2.利用求偏导法则，一层一层的求偏导</p>
<ul>
<li><p>使用类似 tensorflow 的 Autograd 的计算图，将每一个参数变量加入<code>计算图</code>中，然后可以找到变量之间的关联然后求导(实现过于复杂，对于简单的全链接网络没必要这样做)</p>
</li>
<li><p>使用高等数学中的多变量求导，结合线性代数的矩阵变换进行 实数 对 矩阵 的  链式求导</p>
</li>
</ul>
<p><strong>参考</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24863977">矩阵求导术（下）</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24709748">矩阵求导术（上）</a></p>
<p><em>引入克罗内克积实现矩阵的链式求导，在注释代码中是没经过花间的Kron积，运算量极大，经过化简得到简单的矩阵表达式</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">back_propagation</span>(<span class="params">W, A_prev, dZ</span>):</span><br><span class="line">    <span class="comment"># dZA_prev = np.diag(   (A_prev * (1 - A_prev)).T.flatten()  )</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># I = np.eye(A_prev.shape[1])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dZ_prev = np.dot(np.dot(dZA_prev,np.kron(I, W.T)), dZ)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># I = np.eye(W.shape[0])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dW = np.dot(np.kron(A_prev, I),dZ)</span></span><br><span class="line">    <span class="comment"># print(A_prev.shape, W.shape, dZ.shape)</span></span><br><span class="line">    </span><br><span class="line">    dZ_prev = np.multiply( A_prev * (<span class="number">1</span> - A_prev), np.dot(W.T,dZ))</span><br><span class="line">    </span><br><span class="line">    dW = np.dot(dZ,A_prev.T)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dZ_prev, dW</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L_back_propagation</span>(<span class="params">cache, parameters, y</span>):</span><br><span class="line">    w = parameters[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">    layers = parameters[<span class="string">&quot;layers&quot;</span>]</span><br><span class="line">    L = <span class="built_in">len</span>(layers) - <span class="number">1</span></span><br><span class="line">    A = cache[<span class="string">&quot;A&quot;</span>]</span><br><span class="line">    dW_list = <span class="built_in">list</span>()</span><br><span class="line">    dZ_prev = A[L] * (<span class="number">1</span> - A[L]) * (A[L] - y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">0</span>, L)):</span><br><span class="line">        dZ_prev, dW = back_propagation(w[i], A[i], dZ_prev)</span><br><span class="line">        dW_list.append(dW)</span><br><span class="line">    cache[<span class="string">&quot;dW&quot;</span>] = dW_list</span><br><span class="line">    <span class="keyword">return</span> cache</span><br></pre></td></tr></table></figure>

<h2 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h2><p>pass</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">cache, parameters, learning_rate</span>):</span><br><span class="line">    w = parameters[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">    layers = parameters[<span class="string">&quot;layers&quot;</span>]</span><br><span class="line">    L = <span class="built_in">len</span>(layers) - <span class="number">1</span></span><br><span class="line">    dW = cache[<span class="string">&quot;dW&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        w[i] -= dW[L-i-<span class="number">1</span>] * learning_rate</span><br><span class="line">        <span class="comment"># b[i] = b[i] - db[len(b)-i-1] * learning_rate</span></span><br><span class="line">    parameters[<span class="string">&quot;w&quot;</span>] = w</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<h2 id="evaulate"><a href="#evaulate" class="headerlink" title="evaulate"></a>evaulate</h2><p><strong>使用准确度作为评估标准</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">Z, Y</span>):</span><br><span class="line">    bools = Z == Y</span><br><span class="line">    accuracy = np.<span class="built_in">sum</span>(np.reshape(bools,bools.size))/Y.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_detial</span>(<span class="params">y_t, y_p</span>):</span><br><span class="line">    </span><br><span class="line">    y_test = copy.deepcopy(y_t)</span><br><span class="line">    y_pred = copy.deepcopy(y_p)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The roc_auc_score is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(roc_auc_score(y_test, y_pred)))    </span><br><span class="line">    tpr,fpr,thresholds = roc_curve(y_test,y_pred)</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    plt.plot(fpr, tpr)</span><br><span class="line">    plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">    plt.ylim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">    plt.title(<span class="string">&#x27;ROC curve for diabetes classifier&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;False Positive Rate (1 - Specificity)&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;True Positive Rate (Sensitivity)&#x27;</span>)</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># mean = np.sum(y_pred)/y_pred.shape[0]</span></span><br><span class="line">    y_pred[(y_pred &lt;  <span class="number">0.5</span>)] = <span class="number">0</span></span><br><span class="line">    y_pred[(y_pred &gt;= <span class="number">0.5</span>)] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># plt.scatter(x_test[0, :], x_test[1, :], c = y_pred[:, 0])</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The accuracy is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(accuracy_score(y_test[:,<span class="number">0</span>], y_pred[:,<span class="number">0</span>])))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算召回， 查全率， 查准率 。。。。</span></span><br><span class="line">    target_names = [<span class="string">&#x27;class0&#x27;</span>,<span class="string">&#x27;class1&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(classification_report(y_test,y_pred,target_names = target_names))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">## 绘图</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Plot</span>(<span class="params">X, label, parameters</span>):</span><br><span class="line"></span><br><span class="line">    x_min, x_max = X[<span class="number">0</span>,:].<span class="built_in">min</span>() - <span class="number">.5</span>, X[<span class="number">0</span>,:].<span class="built_in">max</span>() + <span class="number">.5</span></span><br><span class="line">    y_min, y_max = X[<span class="number">1</span>,:].<span class="built_in">min</span>() - <span class="number">.5</span>, X[<span class="number">1</span>,:].<span class="built_in">max</span>() + <span class="number">.5</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">    Z = predict(parameters, np.c_[xx.ravel(), yy.ravel(), np.ones((yy.ravel().shape))].T)</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    </span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.scatter(X[<span class="number">0</span>,:],X[<span class="number">1</span>,:],c=label[<span class="number">0</span>,:])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="load-data"><a href="#load-data" class="headerlink" title="load_data"></a>load_data</h2><p><em><strong>加载数据的管道</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">shape = <span class="string">&quot;circle&quot;</span></span>):</span><br><span class="line">    X = np.random.rand(<span class="number">2</span>,<span class="number">200</span>)</span><br><span class="line">    one = np.ones((<span class="number">1</span>,<span class="number">200</span>))</span><br><span class="line">    X = np.vstack((X,one))</span><br><span class="line">    C = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> X.T:</span><br><span class="line">        <span class="keyword">if</span> shape == <span class="string">&quot;circle&quot;</span>:</span><br><span class="line">            <span class="comment"># 同心圆</span></span><br><span class="line">            <span class="keyword">if</span> (x[<span class="number">0</span>]-<span class="number">.5</span>)**<span class="number">2</span> + (x[<span class="number">1</span>]-<span class="number">.5</span>)**<span class="number">2</span> &lt; <span class="number">.03</span>:</span><br><span class="line">                C.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">elif</span> (x[<span class="number">0</span>]-<span class="number">.5</span>)**<span class="number">2</span> + (x[<span class="number">1</span>]-<span class="number">.5</span>)**<span class="number">2</span> &lt; <span class="number">.13</span>:</span><br><span class="line">                C.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                C.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> shape == <span class="string">&quot;xor&quot;</span>:</span><br><span class="line">            <span class="comment"># 四分</span></span><br><span class="line">            <span class="keyword">if</span> (x[<span class="number">0</span>] - <span class="number">.5</span>)*(x[<span class="number">1</span>] - <span class="number">.5</span>) &gt;=<span class="number">0</span>:</span><br><span class="line">                C.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                C.append(<span class="number">0</span>)</span><br><span class="line">                </span><br><span class="line">    x_min, x_max = X[<span class="number">0</span>,:].<span class="built_in">min</span>() - <span class="number">.05</span>, X[<span class="number">0</span>,:].<span class="built_in">max</span>() + <span class="number">.05</span></span><br><span class="line">    y_min, y_max = X[<span class="number">1</span>,:].<span class="built_in">min</span>() - <span class="number">.05</span>, X[<span class="number">1</span>,:].<span class="built_in">max</span>() + <span class="number">.05</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">    test = np.c_[xx.ravel(), yy.ravel(), np.ones((yy.ravel().shape))].T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X, np.array(C).reshape(<span class="number">1</span>,-<span class="number">1</span>), test, (xx,yy)</span><br></pre></td></tr></table></figure>

<h2 id="Back-Propagation-BP算法的整体框架"><a href="#Back-Propagation-BP算法的整体框架" class="headerlink" title="Back Propagation(BP算法的整体框架)"></a>Back Propagation(BP算法的整体框架)</h2><ul>
<li>使用梯度下降的优化方法</li>
<li>其他可用的优化方法<ul>
<li>SGD 随机梯度下降</li>
<li>MBGD （Mini Batch Gradient Descent）用于样本容量大，内存&#x2F;现存不够的情况</li>
<li>Momentum 动量梯度下降</li>
<li>Nesterov NAG</li>
<li>Adagrad</li>
<li>Adaelta</li>
<li>Adam</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">X, Y, test,canvs, layers, learning_rate = <span class="number">0.01</span>, epoch = <span class="number">1000</span>, detial = <span class="literal">True</span>, draw = <span class="literal">False</span></span>):</span><br><span class="line">    parameters = initial(layers)</span><br><span class="line">    draw_param = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        cache = L_layers_propagation(parameters, X)</span><br><span class="line">        cache = L_back_propagation(cache, parameters, Y)</span><br><span class="line">        parameters = update_parameters(cache, parameters, learning_rate)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span> <span class="keyword">and</span> draw == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># draw_param.append(copy.deepcopy(parameters))</span></span><br><span class="line">            <span class="keyword">yield</span> predict(parameters, test, draw = <span class="literal">True</span>).reshape(canvs[<span class="number">0</span>].shape)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span> <span class="keyword">and</span> detial == <span class="literal">True</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;this is &#123;&#125;th epoch.&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    Y_predict = predict(parameters, X, draw=<span class="literal">False</span>)</span><br><span class="line">    accuracy = evaluate(Y, Y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;accuracy is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(accuracy))</span><br><span class="line">    <span class="keyword">if</span> draw == <span class="literal">False</span>:</span><br><span class="line">        <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<h3 id="实现累计BP算法"><a href="#实现累计BP算法" class="headerlink" title="实现累计BP算法"></a>实现累计BP算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">model_caculate</span>(<span class="params">X, Y, X_test, Y_test, layers, learning_rate = <span class="number">0.01</span>, epoch = <span class="number">1000</span>, interval = <span class="number">50</span></span>):</span><br><span class="line">    parameters = initial(layers)</span><br><span class="line">    Loss = []</span><br><span class="line">    loss = <span class="literal">None</span></span><br><span class="line">    Acc = []</span><br><span class="line">    accuracy = <span class="literal">None</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    Y_predict = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        cache = L_layers_propagation(parameters, X)</span><br><span class="line">        cache = L_back_propagation(cache, parameters, Y)</span><br><span class="line">        loss = lossFunc(Y, cache[<span class="string">&quot;A&quot;</span>][-<span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        parameters = update_parameters(cache, parameters, learning_rate)</span><br><span class="line">        <span class="keyword">if</span> i % interval == <span class="number">0</span>:</span><br><span class="line">            Loss.append(loss[<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">            Y_predict = predict(parameters, X_test, draw=<span class="literal">False</span>)</span><br><span class="line">            accuracy = evaluate(Y_test, Y_predict)</span><br><span class="line">            Acc.append(accuracy)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % (interval * <span class="number">50</span>) == <span class="number">0</span>:</span><br><span class="line">                now = time.time()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Calculate Dense Net:~$ &#123;&#125;th epoch,  accuracy: &#123;&#125;,   loss:&#123;&#125;,  time:&#123;&#125;-&quot;</span>.<span class="built_in">format</span>(i, accuracy, loss[<span class="number">0</span>,<span class="number">0</span>], now - start))</span><br><span class="line">                start = time.time()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Loss, Acc, Y_predict</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">X, label, test, canvs = load_data()</span><br><span class="line"></span><br><span class="line">layers = [<span class="number">3</span>,<span class="number">6</span>,<span class="number">9</span>,<span class="number">9</span>,<span class="number">6</span>,<span class="number">1</span>]</span><br><span class="line">epoch = <span class="number">25000</span></span><br><span class="line">Loss, accuracy, Y_pred = model_caculate(X, label, X, label, layers, learning_rate=<span class="number">0.01</span>, epoch= epoch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#开始画图</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">25000</span>, <span class="number">500</span>)</span><br><span class="line">plt.plot(x, np.array(Loss)/(<span class="built_in">max</span>(Loss) - <span class="built_in">min</span>(Loss)), color=<span class="string">&#x27;green&#x27;</span>, label=<span class="string">&#x27;training loss&#x27;</span>)</span><br><span class="line">plt.plot(x, np.array(accuracy)/(<span class="built_in">max</span>(accuracy) - <span class="built_in">min</span>(accuracy)), color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;training accuracy&#x27;</span>)</span><br><span class="line"><span class="comment"># 显示图例</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">&#x27;iteration times&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;rate&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">evaluate_detial(label.T, Y_pred.T)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>Calculate Dense Net:~$ 0th epoch,  accuracy: 0.725,   loss:21.40100100371333,  time:0.0010008811950683594-
Calculate Dense Net:~$ 2500th epoch,  accuracy: 0.725,   loss:19.748958341789084,  time:0.7027204036712646-
Calculate Dense Net:~$ 5000th epoch,  accuracy: 0.725,   loss:18.484585855571503,  time:0.7009453773498535-
Calculate Dense Net:~$ 7500th epoch,  accuracy: 0.705,   loss:16.377806924887945,  time:0.6996634006500244-
Calculate Dense Net:~$ 10000th epoch,  accuracy: 0.87,   loss:10.950215467935237,  time:0.7041025161743164-
Calculate Dense Net:~$ 12500th epoch,  accuracy: 0.88,   loss:9.160948492962692,  time:0.701648473739624-
Calculate Dense Net:~$ 15000th epoch,  accuracy: 0.885,   loss:8.84026852930535,  time:0.6946816444396973-
Calculate Dense Net:~$ 17500th epoch,  accuracy: 0.965,   loss:6.105461022050368,  time:0.685279369354248-
Calculate Dense Net:~$ 20000th epoch,  accuracy: 0.99,   loss:2.3057686501449832,  time:0.7090015411376953-
Calculate Dense Net:~$ 22500th epoch,  accuracy: 0.99,   loss:0.958385284049958,  time:0.6965689659118652-
</code></pre>
<p><img src="C:/Users/herrn/Downloads/1185500/output_36_1.png" alt="png"></p>
<pre><code>The roc_auc_score is 1.0
The accuracy is 1.0
              precision    recall  f1-score   support

      class0       1.00      1.00      1.00        55
      class1       1.00      1.00      1.00       145

   micro avg       1.00      1.00      1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200
</code></pre>
<p><img src="C:/Users/herrn/Downloads/1185500/output_36_3.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="实现标准BP算法（也就是Mini-batch-1，这样的话在python下使用for循环喂数据会非常慢）"><a href="#实现标准BP算法（也就是Mini-batch-1，这样的话在python下使用for循环喂数据会非常慢）" class="headerlink" title="实现标准BP算法（也就是Mini-batch &#x3D; 1，这样的话在python下使用for循环喂数据会非常慢）"></a>实现标准BP算法（也就是Mini-batch &#x3D; 1，这样的话在python下使用for循环喂数据会非常慢）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">model_std</span>(<span class="params">X, Y, X_test, Y_test, layers, learning_rate = <span class="number">0.01</span>, epoch = <span class="number">1000</span>, interval = <span class="number">50</span></span>):</span><br><span class="line">    parameters = initial(layers)</span><br><span class="line">    Loss = []</span><br><span class="line">    Acc = []</span><br><span class="line">    accuracy = <span class="literal">None</span></span><br><span class="line">    loss = <span class="literal">None</span></span><br><span class="line">    Y_pred = <span class="literal">None</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, X.shape[<span class="number">1</span>]):</span><br><span class="line">            cache = L_layers_propagation(parameters, X[:, j].reshape(<span class="number">3</span>, -<span class="number">1</span>))</span><br><span class="line">            cache = L_back_propagation(cache, parameters, Y[:, j].reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">            parameters = update_parameters(cache, parameters, learning_rate)</span><br><span class="line">        <span class="keyword">if</span> i % interval == <span class="number">0</span>:</span><br><span class="line">            Y_pred = predict(parameters, X_test, draw = <span class="literal">False</span>)</span><br><span class="line">            </span><br><span class="line">            loss = lossFunc(Y_test, Y_pred)[<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">            Loss.append(loss)</span><br><span class="line">            accuracy = evaluate(Y_test, Y_pred)</span><br><span class="line">            Acc.append(accuracy)</span><br><span class="line">            <span class="keyword">if</span> i % (interval * <span class="number">20</span>) == <span class="number">0</span>:</span><br><span class="line">                now = time.time()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Stander Dense Net:~$  &#123;&#125;th epoch,  accuracy: &#123;&#125;,   loss:&#123;&#125;,  time:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, accuracy, loss, now - start))</span><br><span class="line">                start = time.time()</span><br><span class="line">    <span class="keyword">return</span> Loss, Acc, Y_pred</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X, label, test, canvs = load_data()</span><br><span class="line">layers = [<span class="number">3</span>,<span class="number">6</span>,<span class="number">9</span>,<span class="number">9</span>,<span class="number">6</span>,<span class="number">1</span>]</span><br><span class="line">epoch = <span class="number">33300</span></span><br><span class="line">Loss,accuracy, Y_pred = model_std(X, label, X, label, layers, learning_rate=<span class="number">0.01</span>, epoch= epoch)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始画图</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">25000</span>, epoch/<span class="number">50</span>)</span><br><span class="line">plt.plot(x, np.array(Loss)/(<span class="built_in">max</span>(Loss) - <span class="built_in">min</span>(Loss)), color=<span class="string">&#x27;green&#x27;</span>, label=<span class="string">&#x27;training loss&#x27;</span>)</span><br><span class="line">plt.plot(x, np.array(accuracy)/(<span class="built_in">max</span>(accuracy) - <span class="built_in">min</span>(accuracy)), color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;training accuracy&#x27;</span>)</span><br><span class="line"><span class="comment"># 显示图例</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">&#x27;iteration times&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;rate&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">evaluate_detial(label.T, Y_pred.T)</span><br></pre></td></tr></table></figure>

<pre><code>Stander Dense Net:~$  0th epoch,  accuracy: 0.65,   loss:35.0,  time:0.018254756927490234
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="动态可视化训练过程"><a href="#动态可视化训练过程" class="headerlink" title="动态可视化训练过程"></a>动态可视化训练过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">epoch= <span class="number">50000</span>, detial=<span class="literal">True</span>, draw=<span class="literal">False</span></span>):   </span><br><span class="line">    X, label, test, canvs = load_data()</span><br><span class="line">    layers = [<span class="number">3</span>,<span class="number">6</span>,<span class="number">9</span>,<span class="number">9</span>,<span class="number">6</span>,<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> draw == <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> im <span class="keyword">in</span> model(X, label, test, canvs, layers, learning_rate=<span class="number">0.01</span>, epoch= epoch, detial = detial, draw = <span class="literal">True</span>):</span><br><span class="line">            plt.clf()</span><br><span class="line">            display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line">            plt.contourf(canvs[<span class="number">0</span>] ,canvs[<span class="number">1</span>], im, cmap=plt.cm.Spectral)</span><br><span class="line">            plt.scatter(X[<span class="number">0</span>,:], X[<span class="number">1</span>,:],c=label[<span class="number">0</span>,:])</span><br><span class="line">            plt.pause(<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        parameters = model(X, label, test, canvs, layers, learning_rate=<span class="number">0.01</span>, epoch= <span class="number">30000</span>, detial = detial, draw = draw)</span><br><span class="line">        Plot(X, label, parameters)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">main(epoch= <span class="number">20000</span>, detial=<span class="literal">False</span>, draw=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h1 id="西瓜数据集3-0-alpha-上的分类"><a href="#西瓜数据集3-0-alpha-上的分类" class="headerlink" title="西瓜数据集3.0$\alpha$上的分类"></a>西瓜数据集3.0$\alpha$上的分类</h1><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load</span>(): </span><br><span class="line">    data = np.array([[<span class="number">0.697</span>, <span class="number">0.460</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0.774</span>, <span class="number">0.376</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0.634</span>, <span class="number">0.264</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0.608</span>, <span class="number">0.318</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0.556</span>, <span class="number">0.215</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0.403</span>, <span class="number">0.237</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0.481</span>, <span class="number">0.149</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0.437</span>, <span class="number">0.211</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0.666</span>, <span class="number">0.091</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.243</span>, <span class="number">0.267</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.245</span>, <span class="number">0.057</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.343</span>, <span class="number">0.099</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.639</span>, <span class="number">0.161</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.657</span>, <span class="number">0.198</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.360</span>, <span class="number">0.370</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.593</span>, <span class="number">0.042</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.719</span>, <span class="number">0.103</span>, <span class="number">0</span>]])</span><br><span class="line">    </span><br><span class="line">    Y = copy.deepcopy(data[:, <span class="number">2</span>].reshape(-<span class="number">1</span>, <span class="number">1</span>).T)</span><br><span class="line">    data[:, <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">    X = copy.deepcopy(data.T)</span><br><span class="line">    </span><br><span class="line">    X_test = copy.deepcopy(np.c_[X[:, <span class="number">0</span>:<span class="number">2</span>],X[:, -<span class="number">3</span>:-<span class="number">1</span>]])</span><br><span class="line">    Y_test = copy.deepcopy(np.c_[Y[:, <span class="number">0</span>:<span class="number">2</span>],Y[:, -<span class="number">3</span>:-<span class="number">1</span>]])</span><br><span class="line">    </span><br><span class="line">    X_train = copy.deepcopy(X[:, <span class="number">2</span>:-<span class="number">2</span>])</span><br><span class="line">    Y_train = copy.deepcopy(Y[:, <span class="number">2</span>:-<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_train, Y_train, X_test, Y_test, X, Y</span><br><span class="line">X_train, Y_train, X_test, Y_test, X, Y = load()</span><br><span class="line"><span class="built_in">print</span>(Y.shape)</span><br></pre></td></tr></table></figure>

<h3 id="累计BP"><a href="#累计BP" class="headerlink" title="累计BP"></a>累计BP</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, Y_train, X_test, Y_test, X, Y = load()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train.shape, Y_train.shape, X_test.shape, Y_test.shape</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">layers = [<span class="number">3</span>,<span class="number">5</span>,<span class="number">1</span>]</span><br><span class="line">epoch = <span class="number">5500</span></span><br><span class="line">interval = <span class="number">1</span></span><br><span class="line">Loss, accuracy, Y_pred = model_caculate(X_train, Y_train, X_test, Y_test, layers, learning_rate=<span class="number">0.1</span>, epoch= epoch, interval = interval)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#开始画图</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">25000</span>, epoch/interval)</span><br><span class="line">plt.plot(x, np.array(Loss)/(<span class="built_in">max</span>(Loss) - <span class="built_in">min</span>(Loss)), color=<span class="string">&#x27;green&#x27;</span>, label=<span class="string">&#x27;training loss&#x27;</span>)</span><br><span class="line">plt.plot(x, np.array(accuracy)/(<span class="built_in">max</span>(accuracy) - <span class="built_in">min</span>(accuracy)), color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;training accuracy&#x27;</span>)</span><br><span class="line"><span class="comment"># 显示图例</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">&#x27;iteration times&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;rate&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">evaluate_detial(Y_test.T, Y_pred.T)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X, label, test, canvs = load_data()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape, label.shape</span><br></pre></td></tr></table></figure>

<h3 id="标准BP算法"><a href="#标准BP算法" class="headerlink" title="标准BP算法"></a>标准BP算法</h3><p><strong>由于实现了累计误差的算法， 所以可以将样样本拆分得到mini-batch的神经网络就是标准BP算法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">layers = [<span class="number">3</span>,<span class="number">5</span>,<span class="number">1</span>]</span><br><span class="line">epoch = <span class="number">5500</span></span><br><span class="line">interval = <span class="number">1</span></span><br><span class="line">Loss,accuracy, Y_pred = model_std(X_train, Y_train, X_test, Y_test, layers, learning_rate=<span class="number">0.01</span>, epoch= epoch)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始画图</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">25000</span>, epoch/<span class="number">50</span>)</span><br><span class="line">plt.plot(x, np.array(Loss)/(<span class="built_in">max</span>(Loss) - <span class="built_in">min</span>(Loss)), color=<span class="string">&#x27;green&#x27;</span>, label=<span class="string">&#x27;training loss&#x27;</span>)</span><br><span class="line">plt.plot(x, np.array(accuracy)/(<span class="built_in">max</span>(accuracy) - <span class="built_in">min</span>(accuracy)), color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;training accuracy&#x27;</span>)</span><br><span class="line"><span class="comment"># 显示图例</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">&#x27;iteration times&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;rate&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">evaluate_detial(Y_test.T, Y_pred.T)</span><br></pre></td></tr></table></figure>

<h1 id="模型比较"><a href="#模型比较" class="headerlink" title="模型比较"></a>模型比较</h1><ul>
<li>标准BP算法收敛更快</li>
<li>累计BP在python可以通过矩阵运算向量化加速,在迭代过程中震荡变小</li>
<li>累计BP 更占用内存，而标准BP可以一边读一边运算，收敛速度变慢但是梯度方向更准确</li>
<li>Minibatch &#x3D; n，可以通过调节n的大小使其达到一个合适的值，其收敛速度和迭代的准确性都能提高！</li>
</ul>
<h3 id="神经网络的优势"><a href="#神经网络的优势" class="headerlink" title="神经网络的优势"></a>神经网络的优势</h3><ul>
<li>相对于决策树，线性模型，神经网络可以通过增加层数&#x2F;增加神经元的方式拟合任意非线性函数</li>
<li>在之前的同心圆数据集中，神经网络的表达能力远远优于其他模型</li>
</ul>
<p>\begin{split}<br>&amp; C(F , G) &#x3D; \frac{F \cdot G+(1 - F\otimes G)}{2}\<br>&amp; F\cdot G &#x3D; \vee_U(\mu_F(u_i)\land\mu_G(u_i)) \<br>&amp; F\otimes G &#x3D; \vee_U(\mu_F(u_i)\vee\mu_G(u_i))<br>\end{split}</p>

      </div>
      
        <div class="prev-or-next">
          <div class="post-foot-next">
            
              <a href="/2024/06/02/%E4%B8%80%E7%A7%8D%E7%BC%93%E5%8A%A8%E6%9B%B2%E7%BA%BF/" target="_self">
                <i class="iconfont icon-chevronleft"></i>
                <span>上一页</span>
              </a>
            
          </div>
          <div class="post-attach">
            <span class="post-pubtime">
              <i class="iconfont icon-updatetime mr-10" title="更新时间"></i>
              2025-12-13 06:32:43
            </span>
            
          </div>
          <div class="post-foot-prev">
            
              <a href="/2024/06/02/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/" target="_self">
                <span>下一页</span>
                <i class="iconfont icon-chevronright"></i>
              </a>
            
          </div>
        </div>
      
    </div>
    
  <div id="btn-catalog" class="btn-catalog">
    <i class="iconfont icon-catalog"></i>
  </div>
  <div class="post-catalog hidden" id="catalog">
    <div class="title">目录</div>
    <div class="catalog-content">
      
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Neural-Network"><span class="toc-text">Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%EF%BC%9A%E9%87%87%E7%94%A8BP%E7%AE%97%E6%B3%95%EF%BC%8C%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%9D%A5%E4%BC%98%E5%8C%96W%EF%BC%8Cb"><span class="toc-text">实现方式：采用BP算法，使用梯度下降来优化W，b</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9C%80%E8%A6%81%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%87%BD%E6%95%B0%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="toc-text">需要实现的函数如下：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E5%81%8F%E5%BE%AE%E5%88%86-%E6%8E%A8%E5%AF%BC%EF%BC%88%E8%BF%99%E9%87%8C%E6%98%AF%E6%A0%B9%E6%8D%AE%E8%A5%BF%E7%93%9C%E4%B9%A6%E4%B8%8A%E5%8E%9F%E5%A7%8B%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%EF%BC%8C%E6%89%80%E4%BB%A5%E6%98%AF%E7%B4%AF%E8%AE%A1%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="toc-text">链式偏微分 推导（这里是根据西瓜书上原始公式推导，所以是累计神经网络）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%B1%E4%BA%8E%E5%9C%A8%E5%B8%83%E7%BD%AE%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B9%8B%E5%89%8D%E5%B0%B1%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%BA%86%E4%B8%80%E4%B8%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%8C%E6%89%80%E4%BB%A5%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%98%AF%E4%BB%BB%E6%84%8F%E5%B1%82%E6%95%B0%E5%92%8C%E4%BB%BB%E6%84%8F%E6%95%B0%E9%87%8F%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%8C%E6%89%80%E4%BB%A5%E5%B0%B1%E6%94%B9%E4%B8%8B%E5%8F%82%E6%95%B0%E4%BA%A4%E4%BD%9C%E4%B8%9A%E4%BA%86"><span class="toc-text">由于在布置编程作业之前就自己实现了一下神经网络，所以实现的是任意层数和任意数量神经元的神经网络，所以就改下参数交作业了</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%87%BD%E6%95%B0initial-layers"><span class="toc-text">初始化函数initial(layers)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Sigmoid"><span class="toc-text">激活函数 Sigmoid()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E7%BA%BF%E6%80%A7%E4%BC%A0%E6%92%AD"><span class="toc-text">前向线性传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#L%E5%B1%82%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">L层的前向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss%E5%87%BD%E6%95%B0%EF%BC%88%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%EF%BC%89"><span class="toc-text">Loss函数（均方误差）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0"><span class="toc-text">预测函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%87%BD%E6%95%B0"><span class="toc-text">反向传播函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0"><span class="toc-text">更新参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#evaulate"><span class="toc-text">evaulate</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-data"><span class="toc-text">load_data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Back-Propagation-BP%E7%AE%97%E6%B3%95%E7%9A%84%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6"><span class="toc-text">Back Propagation(BP算法的整体框架)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E7%B4%AF%E8%AE%A1BP%E7%AE%97%E6%B3%95"><span class="toc-text">实现累计BP算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E6%A0%87%E5%87%86BP%E7%AE%97%E6%B3%95%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AFMini-batch-1%EF%BC%8C%E8%BF%99%E6%A0%B7%E7%9A%84%E8%AF%9D%E5%9C%A8python%E4%B8%8B%E4%BD%BF%E7%94%A8for%E5%BE%AA%E7%8E%AF%E5%96%82%E6%95%B0%E6%8D%AE%E4%BC%9A%E9%9D%9E%E5%B8%B8%E6%85%A2%EF%BC%89"><span class="toc-text">实现标准BP算法（也就是Mini-batch &#x3D; 1，这样的话在python下使用for循环喂数据会非常慢）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text">动态可视化训练过程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A5%BF%E7%93%9C%E6%95%B0%E6%8D%AE%E9%9B%863-0-alpha-%E4%B8%8A%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-text">西瓜数据集3.0$\alpha$上的分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B4%AF%E8%AE%A1BP"><span class="toc-text">累计BP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E5%87%86BP%E7%AE%97%E6%B3%95"><span class="toc-text">标准BP算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%BE%83"><span class="toc-text">模型比较</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-text">神经网络的优势</span></a></li></ol></li></ol>
      
    </div>
  </div>

  
<script src="/js/catalog.js"></script>




    
      <div class="comments-container">
        







      </div>
    
  </div>


        
<div class="footer">
  <div class="social">
    <ul>
      
        <li>
          
              <a title="github" target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">
                <i class="iconfont icon-github"></i>
              </a>
              
        </li>
        
    </ul>
  </div>
  
    
    <div class="footer-more">
      
        <a target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">Copyright © 2025 Oranges</a>
        
    </div>
  
    
    <div class="footer-more">
      
        <a target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">Theme by Oranges | Powered by Hexo</a>
        
    </div>
  
  
</div>

      </div>

      <div class="tools-bar">
        <div class="back-to-top tools-bar-item hidden">
  <a href="javascript: void(0)">
    <i class="iconfont icon-chevronup"></i>
  </a>
</div>


<script src="/js/backtotop.js"></script>



        
  <div class="search-icon tools-bar-item" id="search-icon">
    <a href="javascript: void(0)">
      <i class="iconfont icon-search"></i>
    </a>
  </div>

  <div class="search-overlay hidden">
    <div class="search-content" tabindex="0">
      <div class="search-title">
        <span class="search-icon-input">
          <a href="javascript: void(0)">
            <i class="iconfont icon-search"></i>
          </a>
        </span>
        
          <input type="text" class="search-input" id="search-input" placeholder="搜索...">
        
        <span class="search-close-icon" id="search-close-icon">
          <a href="javascript: void(0)">
            <i class="iconfont icon-close"></i>
          </a>
        </span>
      </div>
      <div class="search-result" id="search-result"></div>
    </div>
  </div>

  <script type="text/javascript">
    var inputArea = document.querySelector("#search-input")
    var searchOverlayArea = document.querySelector(".search-overlay")

    inputArea.onclick = function() {
      getSearchFile()
      this.onclick = null
    }

    inputArea.onkeydown = function() {
      if(event.keyCode == 13)
        return false
    }

    function openOrHideSearchContent() {
      let isHidden = searchOverlayArea.classList.contains('hidden')
      if (isHidden) {
        searchOverlayArea.classList.remove('hidden')
        document.body.classList.add('hidden')
        // inputArea.focus()
      } else {
        searchOverlayArea.classList.add('hidden')
        document.body.classList.remove('hidden')
      }
    }

    function blurSearchContent(e) {
      if (e.target === searchOverlayArea) {
        openOrHideSearchContent()
      }
    }

    document.querySelector("#search-icon").addEventListener("click", openOrHideSearchContent, false)
    document.querySelector("#search-close-icon").addEventListener("click", openOrHideSearchContent, false)
    searchOverlayArea.addEventListener("click", blurSearchContent, false)

    var searchFunc = function (path, search_id, content_id) {
      'use strict';
      var $input = document.getElementById(search_id);
      var $resultContent = document.getElementById(content_id);
      $resultContent.innerHTML = "<ul><span class='local-search-empty'>首次搜索，正在载入索引文件，请稍后……<span></ul>";
      $.ajax({
        // 0x01. load xml file
        url: path,
        dataType: "xml",
        success: function (xmlResponse) {
          // 0x02. parse xml file
          var datas = $("entry", xmlResponse).map(function () {
            return {
              title: $("title", this).text(),
              content: $("content", this).text(),
              url: $("url", this).text()
            };
          }).get();
          $resultContent.innerHTML = "";

          $input.addEventListener('input', function () {
            // 0x03. parse query to keywords list
            var str = '<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length <= 0) {
              return;
            }
            // 0x04. perform local searching
            datas.forEach(function (data) {
              var isMatch = true;
              var content_index = [];
              if (!data.title || data.title.trim() === '') {
                data.title = "Untitled";
              }
              var orig_data_title = data.title.trim();
              var data_title = orig_data_title.toLowerCase();
              var orig_data_content = data.content.trim().replace(/<[^>]+>/g, "");
              var data_content = orig_data_content.toLowerCase();
              var data_url = data.url;
              var index_title = -1;
              var index_content = -1;
              var first_occur = -1;
              // only match artiles with not empty contents
              if (data_content !== '') {
                keywords.forEach(function (keyword, i) {
                  index_title = data_title.indexOf(keyword);
                  index_content = data_content.indexOf(keyword);

                  if (index_title < 0 && index_content < 0) {
                    isMatch = false;
                  } else {
                    if (index_content < 0) {
                      index_content = 0;
                    }
                    if (i == 0) {
                      first_occur = index_content;
                    }
                    // content_index.push({index_content:index_content, keyword_len:keyword_len});
                  }
                });
              } else {
                isMatch = false;
              }
              // 0x05. show search results
              if (isMatch) {
                str += "<li><a href='" + data_url + "' class='search-result-title'>" + orig_data_title + "</a>";
                var content = orig_data_content;
                if (first_occur >= 0) {
                  // cut out 100 characters
                  var start = first_occur - 20;
                  var end = first_occur + 80;

                  if (start < 0) {
                    start = 0;
                  }

                  if (start == 0) {
                    end = 100;
                  }

                  if (end > content.length) {
                    end = content.length;
                  }

                  var match_content = content.substr(start, end);

                  // highlight all keywords
                  keywords.forEach(function (keyword) {
                    var regS = new RegExp(keyword, "gi");
                    match_content = match_content.replace(regS, "<span class=\"search-keyword\">" + keyword + "</span>");
                  });

                  str += "<p class=\"search-result-abstract\">" + match_content + "...</p>"
                }
                str += "</li>";
              }
            });
            str += "</ul>";
            if (str.indexOf('<li>') === -1) {
              return $resultContent.innerHTML = "<ul><span class='local-search-empty'>没有找到内容，请尝试更换检索词。<span></ul>";
            }
            $resultContent.innerHTML = str;
          });
        },
        error: function(xhr, status, error) {
          $resultContent.innerHTML = ""
          if (xhr.status === 404) {
            $resultContent.innerHTML = "<ul><span class='local-search-empty'>未找到search.xml文件，具体请参考：<a href='https://github.com/zchengsite/hexo-theme-oranges#configuration' target='_black'>configuration</a><span></ul>";
          } else {
            $resultContent.innerHTML = "<ul><span class='local-search-empty'>请求失败，尝试重新刷新页面或稍后重试。<span></ul>";
          }
        }
      });
      $(document).on('click', '#search-close-icon', function() {
        $('#search-input').val('');
        $('#search-result').html('');
      });
    }

    var getSearchFile = function() {
        var path = "/search.xml";
        searchFunc(path, 'search-input', 'search-result');
    }
  </script>




        
  <div class="tools-bar-item theme-icon" id="switch-color-scheme">
    <a href="javascript: void(0)">
      <i id="theme-icon" class="iconfont icon-moon"></i>
    </a>
  </div>

  
<script src="/js/colorscheme.js"></script>





        
  
    <div class="share-icon tools-bar-item">
      <a href="javascript: void(0)" id="share-icon">
        <i class="iconfont iconshare"></i>
      </a>
      <div class="share-content hidden">
        
          <a class="share-item" href="https://twitter.com/intent/tweet?text=' + %E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80 + '&url=' + http%3A%2F%2Fdev.quijoo.site%3A8002%2F2024%2F06%2F02%2F%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%25E5%259F%25BA%25E7%25A1%2580%2F + '" target="_blank" title="Twitter">
            <i class="iconfont icon-twitter"></i>
          </a>
        
        
          <a class="share-item" href="https://www.facebook.com/sharer.php?u=http://dev.quijoo.site:8002/2024/06/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" target="_blank" title="Facebook">
            <i class="iconfont icon-facebooksquare"></i>
          </a>
        
      </div>
    </div>
  
  
<script src="/js/shares.js"></script>



      </div>
    </div>
  </body>
</html>
